### Import Data
import pandas as pd
df_all = pd.read_csv('filtered_df2.csv')
### Data Split
df_all['datex'] = pd.to_datetime(df_all['datex'], errors='coerce')
df_all = df_all[~df_all['datex'].isna()]
# Define ranges
train = df_all[(df_all['datex'] >= '2016-09-26') & (df_all['datex'] <= '2016-10-31')]
dev   = df_all[(df_all['datex'] >= '2016-11-01') & (df_all['datex'] <= '2016-11-05')]
test  = df_all[(df_all['datex'] >= '2016-11-06') & (df_all['datex'] <= '2016-11-08')]

### Basic Cleaning for All Dataset
import re

def basic_clean(text):
    text = re.sub(r'http\S+|www\S+|https\S+', 'http://URL', text)  # Replace URLs
    text = re.sub(r'@\w+', '@user', text)                          # Replace mentions
    return text

# Apply to each
for df in [train_bal, dev, test]:
    df['Tweets'] = df['Tweets'].astype(str).apply(basic_clean)
### Downsample Negative Class
# Separate classes
train_pos = train[train['io_flag'] == 1]
train_neg = train[train['io_flag'] == 0]

# Downsample negative class to 2x positive (1:2 ratio)
train_neg_down = train_neg.sample(n=2 * len(train_pos), random_state=42)

# Merge and shuffle
train_bal = pd.concat([train_pos, train_neg_down]).sample(frac=1, random_state=42).reset_index(drop=True)

train_bal['io_flag'].value_counts()
### Mask Topic-Specific Words
# step 1 : copy train data
train_copy = train_bal.copy()

# step 2: Preprocess text for word frequency analysis
import nltk
import string
from collections import Counter
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_for_freq(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# Apply to all training texts and flatten the token list
all_tokens = train_copy['Tweets'].apply(preprocess_for_freq)
flat_tokens = [token for sublist in all_tokens for token in sublist]


# Step 3: Get Top 50 Most Frequent Words
word_freq = Counter(flat_tokens)
top_words = [word for word, count in word_freq.most_common(50)]

print(top_words)  # <- You may manually inspect or directly use this

# Step 4: Function for word masking
# Identified topic specific words from top_words
topic_words = ['trump', 'clinton', 'hillary', 'donald', 'debate', 'vote', 'debatenight', 'trumps', 'gop', 'president', 'obama', 'election', 'bill', 'america', 'campaign', 'maga']

# Masking using <mask> for BERTweet model
def mask_topic_words_tweet(text):
    for word in topic_words:
        pattern = rf'\b{word}\b'  # word boundary match
        text = re.sub(pattern, '<mask>', text, flags=re.IGNORECASE)
    return text

# Masking using [MASK] for BERTSentence model
def mask_topic_words_user(text):
    for word in topic_words:
        pattern = rf'\b{word}\b'  # word boundary match
        text = re.sub(pattern, '[MASK]', text, flags=re.IGNORECASE)
    return text


# Step 5: apply word masking
train_bal_tweet = train_bal.copy()
train_bal_user = train_bal.copy()

# for tweet level
train_bal_tweet['Tweets'] = train_bal_tweet['Tweets'].apply(mask_topic_words_tweet)

# for user level
train_bal_user['Tweets'] = train_bal_user['Tweets'].apply(mask_topic_words_user)

train_tweet = train.copy()
train_user = train.copy()

# for tweet level
train_tweet['Tweets'] = train_tweet['Tweets'].apply(mask_topic_words_tweet)

# for user level
train_user['Tweets'] = train_user['Tweets'].apply(mask_topic_words_user)
dev_bal_tweet = dev.copy()
dev_bal_user = dev.copy()


# for tweet level
dev_bal_tweet['Tweets'] = dev_bal_tweet['Tweets'].apply(mask_topic_words_tweet)

# for user level
dev_bal_user['Tweets'] = dev_bal_user['Tweets'].apply(mask_topic_words_user)
test_bal_tweet = test.copy()
test_bal_user = test.copy()

# for tweet level
test_bal_tweet['Tweets'] = test_bal_tweet['Tweets'].apply(mask_topic_words_tweet)

# for user level
test_bal_user['Tweets'] = test_bal_user['Tweets'].apply(mask_topic_words_user)
### Export as CSV
# Save to CSV
train_bal_tweet.to_csv('train_cleaned_tweet.csv', index=False)
train_bal_user.to_csv('train_cleaned_user.csv', index=False)

train_tweet.to_csv('new_train_cleaned_tweet.csv', index=False)
train_user.to_csv('new_train_cleaned_user.csv', index=False)


dev_bal_tweet.to_csv('new_dev_cleaned.csv', index=False)
dev_bal_user.to_csv('new_dev_cleaned_user.csv', index=False)

test_bal_tweet.to_csv('new_test_cleaned.csv', index=False)
test_bal_user.to_csv('new_test_cleaned_user.csv', index=False)
