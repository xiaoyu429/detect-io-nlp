{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setup\n",
        "It is advisable to mount a certain gdrive folder to streamline the work"
      ],
      "metadata": {
        "id": "1sQch-YtcHEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define and create the output folder, change the base path as you like\n",
        "base_path = Path('/content/drive/My Drive/ColabOutputs')\n",
        "base_path.mkdir(parents=True, exist_ok=True)  # Create folder if it doesn't exist\n",
        "\n"
      ],
      "metadata": {
        "id": "4bD1PN7zbgeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTweet Classifier (Model 1)"
      ],
      "metadata": {
        "id": "3pxjcbtcEiBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Datasource"
      ],
      "metadata": {
        "id": "2kUBhuyRdNM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0LgBwDoEYK1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(base_path / 'train_cleaned_tweet.csv')\n",
        "dev_df = pd.read_csv(base_path / 'new_dev_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"io_flag\"] = train_df[\"io_flag\"].astype(int)\n",
        "dev_df[\"io_flag\"] = dev_df[\"io_flag\"].astype(int)"
      ],
      "metadata": {
        "id": "3ubvyzUeEf4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Data, Tokenize, and Load Model"
      ],
      "metadata": {
        "id": "2WS_3bsAdQWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define data\n",
        "df_for_hf = train_df[[\"row_id\", \"Tweets\", \"io_flag\"]].rename(columns={\"Tweets\": \"text\", \"io_flag\": \"label\"})\n",
        "dataset = Dataset.from_pandas(df_for_hf, preserve_index=False)\n",
        "\n",
        "dev_df2 = dev_df[[\"row_id\", \"Tweets\", \"io_flag\"]].rename(columns={\"Tweets\": \"text\", \"io_flag\": \"label\"})\n",
        "eval_dataset = Dataset.from_pandas(dev_df2, preserve_index=False)"
      ],
      "metadata": {
        "id": "3L4MIBkZExnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenizer & Model\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# 3. Tokenization function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "93MlyZamE2ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Arguments"
      ],
      "metadata": {
        "id": "nhXf2ZcDdYv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training Arguments\n",
        "# https://huggingface.co/docs/transformers/v4.53.1/en/main_classes/trainer#transformers.TrainingArguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= base_path / \"bertweet-trainer-checkpoints\"\n",
        "    # output_dir=\"./results\",\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=6063,\n",
        "    save_steps=6063,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    save_total_limit=1,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "yDbeUgtsFBSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Evaluation Metrics"
      ],
      "metadata": {
        "id": "d3DNpIK0ddsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    probs = pred.predictions[:, 1]\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"roc_auc\": roc_auc_score(labels, probs),\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "EDe0YAgpFHSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callback Function to Save Model in Every n Steps"
      ],
      "metadata": {
        "id": "JTqEegTNdq2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import TrainerCallback\n",
        "import os\n",
        "\n",
        "class GoogleDriveSaverCallback(TrainerCallback):\n",
        "    def __init__(self, save_path, save_every_steps):\n",
        "        self.save_path = save_path\n",
        "        self.save_every_steps = save_every_steps\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % self.save_every_steps == 0 and state.global_step > 0:\n",
        "            save_dir = os.path.join(self.save_path, f\"step-{state.global_step}\")\n",
        "            kwargs['model'].save_pretrained(save_dir)\n",
        "            print(f\"\\n Model saved to {save_dir}\")\n"
      ],
      "metadata": {
        "id": "hZzKNbNFFJJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Trainer\n",
        "from transformers import Trainer\n",
        "\n",
        "\n",
        "gd_callback = GoogleDriveSaverCallback(\n",
        "    save_path= base_path / \"bertweet-checkpoints\",\n",
        "    save_every_steps=4000  # adjust as needed\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FmqoGBOAFK47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "KYnmpWtJd1eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[gd_callback]\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "TMMef4TqFPnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if needed -- eg collab disconnects suddenly\n",
        "# trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "SBS9XUkGFSVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Evaluation of Best Model on Validation Set"
      ],
      "metadata": {
        "id": "awtacyyVd4Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Final evaluation on dev\n",
        "final_metrics = trainer.evaluate(eval_dataset=tokenized_eval_dataset)\n",
        "print(\"üîç Final metrics on dev set:\")\n",
        "for key, value in final_metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "itKzp9miFWzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Best Model to Gdrive"
      ],
      "metadata": {
        "id": "a7lgEZnJd8tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Define base path and model name\n",
        "model_name = \"bertweet_metrics_f1_v1\"\n",
        "save_path = base_path / model_name\n",
        "\n",
        "# Save model and tokenizer\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"‚úÖ Model and tokenizer saved to: {save_path}\")\n"
      ],
      "metadata": {
        "id": "P-ICDRSGcsya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTweet Embedding + MLP (Model 2)\n",
        "This part can only be run after we run Model 1"
      ],
      "metadata": {
        "id": "ZlkDXE3DFbJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and Import\n",
        "!pip install transformers --quiet\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "s4NQ2GNtFgzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model 1"
      ],
      "metadata": {
        "id": "63KkP98teFwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model location using base path\n",
        "model_name = \"bertweet_metrics_f1_threshold70\"\n",
        "model_path = base_path / model_name\n",
        "\n",
        "# Load tokenizer and model from local path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Use AutoModel if extracting embeddings; use AutoModelForSequenceClassification for classification\n",
        "model = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "jrUxWxVrF1gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Function to Generate Embeddings"
      ],
      "metadata": {
        "id": "g5PDWqDCeJUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_bertweet_embeddings(texts, batch_size=64):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting BERTweet embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encoded = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded)\n",
        "            cls_embeddings = output.last_hidden_state[:, 0, :]  # CLS token\n",
        "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
        "    return np.vstack(all_embeddings)\n"
      ],
      "metadata": {
        "id": "ytAhJfLyF4eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Embeddings on Train and Validation Set"
      ],
      "metadata": {
        "id": "UnmRmit2ePjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full = pd.read_csv(base_path / 'new_train_cleaned_tweet.csv')\n",
        "\n",
        "texts = train_full['Tweets'].tolist()\n",
        "bertweet_embeddings = get_bertweet_embeddings(texts)  # shape (n, 768)"
      ],
      "metadata": {
        "id": "6Kp9_CVWF7Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = dev_df['Tweets'].tolist()\n",
        "bertweet_embeddings_dev = get_bertweet_embeddings(texts)  # shape (n, 768)"
      ],
      "metadata": {
        "id": "fQn7Xd7nGhoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MLP Using Embeddings"
      ],
      "metadata": {
        "id": "o8-Oq4aDeqwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = train_full['io_flag']\n",
        "dev_label = dev_df['io_flag']\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    bertweet_embeddings, train_labe, bertweet_embeddings_dev, dev_label\n",
        ")\n"
      ],
      "metadata": {
        "id": "IReVtcBVGMHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Final MLP\n",
        "meta_model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(768,)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "meta_model.compile(\n",
        "    optimizer=Adam(1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='new_mlp_baseline_{epoch:02d}.keras',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "meta_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "id": "QL3rjILXG5D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Best Model"
      ],
      "metadata": {
        "id": "fw7GU3kzezMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "# Define source and destination paths\n",
        "source = 'mlp_baseline_epoch_09.keras'\n",
        "destination = base_path / 'new_mlp_baseline_09.keras'\n",
        "\n",
        "# Copy the file\n",
        "shutil.copy(source, destination)"
      ],
      "metadata": {
        "id": "PgKL_LCzG5uQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}