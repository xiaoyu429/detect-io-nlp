{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setup\n",
        "It is advisable to mount a certain gdrive folder to streamline the work"
      ],
      "metadata": {
        "id": "zffRi-qDa9vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define and create the output folder\n",
        "base_path = Path('/content/drive/My Drive/ColabOutputs')\n",
        "base_path.mkdir(parents=True, exist_ok=True)  # Create folder if it doesn't exist\n",
        "\n"
      ],
      "metadata": {
        "id": "W6sXWdUpa7JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUmja62mXFEt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "EFKJ5W6Ue-lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(base_path / \"train_cleaned_user.csv\")\n",
        "df['row_id'] = np.arange(len(df))\n",
        "df['Tweets'] = df['Tweets'].fillna('')\n",
        "df['datex'] = pd.to_datetime(df['datex'])"
      ],
      "metadata": {
        "id": "XdZxfKOFXRqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Embeddings Using BERTSentence"
      ],
      "metadata": {
        "id": "W5oY83ZifAVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SentenceBERT embedding\n",
        "user_grouped = df.groupby('User')['Tweets'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "user_grouped['embedding'] = user_grouped['Tweets'].apply(lambda x: model.encode(x))\n",
        "embedding_matrix = np.vstack(user_grouped['embedding'].values)"
      ],
      "metadata": {
        "id": "g6h8LpV5XWAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for User Metrics"
      ],
      "metadata": {
        "id": "yz-B9HwhfUFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post count per active day\n",
        "def active_days(dates):\n",
        "    return len(set(dates.dt.date))\n",
        "\n",
        "user_dates = df.groupby('User')['datex'].apply(list).reset_index()\n",
        "user_dates['active_days'] = user_dates['datex'].apply(lambda dates: active_days(pd.Series(dates)))\n",
        "user_dates['post_count'] = df.groupby('User').size().values\n",
        "user_dates['avg_post_per_day'] = user_dates['post_count'] / user_dates['active_days']\n",
        "user_grouped = user_grouped.merge(user_dates[['User', 'avg_post_per_day']], on='User')\n",
        "\n",
        "# Burstiness and average tweet length\n",
        "def compute_burstiness(dates):\n",
        "    if len(dates) < 2:\n",
        "        return 0\n",
        "    dates_sorted = sorted(dates)\n",
        "    gaps = [(dates_sorted[i+1] - dates_sorted[i]).total_seconds() / 3600 for i in range(len(dates_sorted)-1)]\n",
        "    return np.std(gaps)\n",
        "\n",
        "burstiness_df = df.groupby('User')['datex'].apply(compute_burstiness).reset_index(name='burstiness')\n",
        "avg_len_df = df.groupby('User')['Tweets'].apply(lambda x: np.mean(x.str.len())).reset_index(name='avg_tweet_length')\n",
        "behavior_df = burstiness_df.merge(avg_len_df, on='User')\n",
        "user_grouped = user_grouped.merge(behavior_df, on='User')"
      ],
      "metadata": {
        "id": "ILVx7MLEfTf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "np.save(base_path / 'user_embeddings.npy', embedding_matrix)\n",
        "user_grouped.drop(columns='embedding').to_csv(base_path / 'user_features.csv', index=False)\n",
        "behavior_df.to_csv(base_path / 'user_behavior_features.csv', index=False)"
      ],
      "metadata": {
        "id": "R0aDkw4-XYh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "user_grouped = df.groupby('User')['Tweets'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# avg_post_per_day\n",
        "def active_days(dates):\n",
        "    return len(set(dates.dt.date))\n",
        "user_dates = df.groupby('User')['datex'].apply(list).reset_index()\n",
        "user_dates['active_days'] = user_dates['datex'].apply(lambda d: active_days(pd.Series(d)))\n",
        "user_dates['post_count'] = df.groupby('User').size().values\n",
        "user_dates['avg_post_per_day'] = user_dates['post_count'] / user_dates['active_days']\n",
        "user_grouped = user_grouped.merge(user_dates[['User', 'avg_post_per_day']], on='User')\n",
        "\n",
        "# Burstiness & avg_len\n",
        "def compute_burstiness(dates):\n",
        "    if len(dates) < 2:\n",
        "        return 0\n",
        "    dates_sorted = sorted(dates)\n",
        "    gaps = [(dates_sorted[i+1] - dates_sorted[i]).total_seconds() / 3600 for i in range(len(dates_sorted)-1)]\n",
        "    return np.std(gaps)\n",
        "burstiness_df = df.groupby('User')['datex'].apply(compute_burstiness).reset_index(name='burstiness')\n",
        "avg_len_df = df.groupby('User')['Tweets'].apply(lambda x: np.mean(x.str.len())).reset_index(name='avg_tweet_length')\n",
        "behavior_df = burstiness_df.merge(avg_len_df, on='User')\n",
        "user_grouped = user_grouped.merge(behavior_df, on='User')\n",
        "\n",
        "labels = df[['User', 'io_flag']].dropna().drop_duplicates()\n",
        "labels = labels.groupby('User')['io_flag'].max().reset_index()\n",
        "user_grouped = user_grouped.merge(labels, on='User')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vAWMd4v0Xhcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_grouped.to_csv(base_path / \"user_features_labeled.csv\", index=False)"
      ],
      "metadata": {
        "id": "DkN2bcujXnEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "-ir56LoafdF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(base_path / \"user_features_labeled.csv\")\n",
        "X_train = train[['avg_post_per_day', 'burstiness', 'avg_tweet_length']]\n",
        "y_train = train['io_flag']\n",
        "\n",
        "model = RandomForestClassifier(class_weight={0:1, 1:20},random_state=42)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "fgaH-tvlXrIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.drop(columns=[col for col in test.columns if 'io_flag' in col])\n",
        "eval_df = test.merge(true_labels, on='User')\n",
        "print(eval_df.columns)"
      ],
      "metadata": {
        "id": "tnouMzAVXzTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "\n",
        "auc = roc_auc_score(eval_df['io_flag'], eval_df['io_prob'])\n",
        "eval_df['pred_label'] = (eval_df['io_prob'] >= 0.7).astype(int)\n",
        "precision = precision_score(eval_df['io_flag'], eval_df['pred_label'])\n",
        "recall = recall_score(eval_df['io_flag'], eval_df['pred_label'])\n",
        "f1 = f1_score(eval_df['io_flag'], eval_df['pred_label'])\n",
        "\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "pObqlt6BXv80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Model"
      ],
      "metadata": {
        "id": "qES_VvtXfiU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "# Define base path and model name\n",
        "model_name = \"user_model_v1\"\n",
        "\n",
        "# Full save path\n",
        "save_path = base_path / model_name\n",
        "\n",
        "# Save and reload model\n",
        "joblib.dump(model, save_path)\n",
        "model = joblib.load(save_path)\n",
        "\n",
        "print(f\"âœ… Model and tokenizer saved to: {save_path}\")\n"
      ],
      "metadata": {
        "id": "x0Eo13T2bR1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}