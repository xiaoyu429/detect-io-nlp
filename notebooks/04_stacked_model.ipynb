{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setup\n",
        "It is advisable to mount a certain gdrive folder to streamline the work"
      ],
      "metadata": {
        "id": "m7REEpzPXJR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define and create the output folder\n",
        "base_path = Path('/content/drive/My Drive/ColabOutputs')\n",
        "base_path.mkdir(parents=True, exist_ok=True)  # Create folder if it doesn't exist\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIc6MLA6XCW_",
        "outputId": "68e58590-7969-4ade-e184-b2dce8fe2e54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read File"
      ],
      "metadata": {
        "id": "lPJLXT_1Xck8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hkXDRjZIX955"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(base_path / \"new_train_cleaned_tweet.csv\")\n",
        "dev_df = pd.read_csv(base_path / \"new_dev_cleaned.csv\")\n",
        "test_df = pd.read_csv(base_path / \"new_test_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna(subset=['io_flag'])"
      ],
      "metadata": {
        "id": "kZLTtW74YMlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"io_flag\"] = train_df[\"io_flag\"].astype(int)\n",
        "dev_df[\"io_flag\"] = dev_df[\"io_flag\"].astype(int)"
      ],
      "metadata": {
        "id": "rGWaMIIGYSHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Metrics Extraction"
      ],
      "metadata": {
        "id": "nwcGyYPNYVzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "TQqbXzx-YSzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(base_path / \"new_train_cleaned_user.csv\")\n",
        "df['row_id'] = np.arange(len(df))\n",
        "df['Tweets'] = df['Tweets'].fillna('')\n",
        "df['datex'] = pd.to_datetime(df['datex'])\n",
        "df = df.dropna(subset=['io_flag'])"
      ],
      "metadata": {
        "id": "3hxsJt8rYb_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentenceBERT embedding\n",
        "user_grouped = df.groupby('User')['Tweets'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "user_grouped['embedding'] = user_grouped['Tweets'].apply(lambda x: model.encode(x))\n",
        "embedding_matrix = np.vstack(user_grouped['embedding'].values)"
      ],
      "metadata": {
        "id": "3hw6N3ZefoVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post count per active day\n",
        "def active_days(dates):\n",
        "    return len(set(dates.dt.date))\n",
        "\n",
        "user_dates = df.groupby('User')['datex'].apply(list).reset_index()\n",
        "user_dates['active_days'] = user_dates['datex'].apply(lambda dates: active_days(pd.Series(dates)))\n",
        "user_dates['post_count'] = df.groupby('User').size().values\n",
        "user_dates['avg_post_per_day'] = user_dates['post_count'] / user_dates['active_days']\n",
        "user_grouped = user_grouped.merge(user_dates[['User', 'avg_post_per_day']], on='User')\n",
        "\n",
        "# Burstiness and average tweet length\n",
        "def compute_burstiness(dates):\n",
        "    if len(dates) < 2:\n",
        "        return 0\n",
        "    dates_sorted = sorted(dates)\n",
        "    gaps = [(dates_sorted[i+1] - dates_sorted[i]).total_seconds() / 3600 for i in range(len(dates_sorted)-1)]\n",
        "    return np.std(gaps)\n",
        "\n",
        "burstiness_df = df.groupby('User')['datex'].apply(compute_burstiness).reset_index(name='burstiness')\n",
        "avg_len_df = df.groupby('User')['Tweets'].apply(lambda x: np.mean(x.str.len())).reset_index(name='avg_tweet_length')\n",
        "behavior_df = burstiness_df.merge(avg_len_df, on='User')\n",
        "user_grouped = user_grouped.merge(behavior_df, on='User')\n",
        "\n"
      ],
      "metadata": {
        "id": "N99wKv15YfeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "np.save(base_path / 'new_user_embeddings.npy', embedding_matrix)\n",
        "user_grouped.drop(columns='embedding').to_csv(base_path / 'new_user_features.csv', index=False)\n",
        "behavior_df.to_csv(base_path / 'new_user_behavior_features.csv', index=False)\n"
      ],
      "metadata": {
        "id": "_RYSU4l2YgZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(base_path / \"new_train_cleaned_user.csv\")\n",
        "df['Tweets'] = df['Tweets'].fillna('')\n",
        "df['datex'] = pd.to_datetime(df['datex'])\n",
        "\n",
        "user_grouped = df.groupby('User')['Tweets'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# avg_post_per_day\n",
        "def active_days(dates):\n",
        "    return len(set(dates.dt.date))\n",
        "user_dates = df.groupby('User')['datex'].apply(list).reset_index()\n",
        "user_dates['active_days'] = user_dates['datex'].apply(lambda d: active_days(pd.Series(d)))\n",
        "user_dates['post_count'] = df.groupby('User').size().values\n",
        "user_dates['avg_post_per_day'] = user_dates['post_count'] / user_dates['active_days']\n",
        "user_grouped = user_grouped.merge(user_dates[['User', 'avg_post_per_day']], on='User')\n",
        "\n",
        "# Burstiness & avg_len\n",
        "def compute_burstiness(dates):\n",
        "    if len(dates) < 2:\n",
        "        return 0\n",
        "    dates_sorted = sorted(dates)\n",
        "    gaps = [(dates_sorted[i+1] - dates_sorted[i]).total_seconds() / 3600 for i in range(len(dates_sorted)-1)]\n",
        "    return np.std(gaps)\n",
        "burstiness_df = df.groupby('User')['datex'].apply(compute_burstiness).reset_index(name='burstiness')\n",
        "avg_len_df = df.groupby('User')['Tweets'].apply(lambda x: np.mean(x.str.len())).reset_index(name='avg_tweet_length')\n",
        "behavior_df = burstiness_df.merge(avg_len_df, on='User')\n",
        "user_grouped = user_grouped.merge(behavior_df, on='User')\n",
        "\n",
        "labels = df[['User', 'io_flag']].dropna().drop_duplicates()\n",
        "labels = labels.groupby('User')['io_flag'].max().reset_index()\n",
        "user_grouped = user_grouped.merge(labels, on='User')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uuu1d17PYlB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#('/content/drive/MyDrive/ColabOutputs/train_cleaned_tweet.csv', index=False)\n",
        "user_grouped.to_csv(base_path / \"new_user_features_labeled.csv\", index=False)"
      ],
      "metadata": {
        "id": "-OyFlDjUYlzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(base_path / \"new_user_features_labeled.csv\")\n",
        "user_train = train[['User','avg_post_per_day',\t'burstiness',\t'avg_tweet_length']]\n",
        "combined_train = pd.merge(train_df, user_train, on='User', how='left')"
      ],
      "metadata": {
        "id": "kblDrxgXYoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install emoji package\n",
        "# !pip3 install emoji==0.6.0"
      ],
      "metadata": {
        "id": "aTIKQ1uvYy6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Tweet Level Model"
      ],
      "metadata": {
        "id": "PjR-uLjif3Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and Import\n",
        "# !pip install transformers --quiet\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "_MGiPxuCYqgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model location using base path\n",
        "model_name = \"bertweet_metrics_f1_threshold70\"\n",
        "model_path = base_path / model_name\n",
        "\n",
        "# Load tokenizer and model from local path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Use AutoModel if extracting embeddings; use AutoModelForSequenceClassification for classification\n",
        "model = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "collapsed": true,
        "id": "AZNanpYZY13F",
        "outputId": "63a9a430-ada2-4189-d86c-fc20db16de73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-1498962698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Use AutoModel if extracting embeddings; use AutoModelForSequenceClassification for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m             )\n\u001b[1;32m    596\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2154\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2155\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Function to Generate Embeddings for Tweet Level Model"
      ],
      "metadata": {
        "id": "wBBIhkAqf_W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_bertweet_embeddings(texts, batch_size=64):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting BERTweet embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encoded = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded)\n",
        "            cls_embeddings = output.last_hidden_state[:, 0, :]  # CLS token\n",
        "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
        "    return np.vstack(all_embeddings)\n"
      ],
      "metadata": {
        "id": "oONIE4kaY3KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = combined_train['Tweets'].tolist()\n",
        "bertweet_embeddings = get_bertweet_embeddings(texts)  # shape (n, 768)"
      ],
      "metadata": {
        "id": "H3pgEWOCY7UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(base_path /'bertweet_embeddings.npy', bertweet_embeddings)"
      ],
      "metadata": {
        "id": "gI4FEgJXY-cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if needed\n",
        "# bertweet_embeddings = np.load(base_path /'bertweet_embeddings.npy')"
      ],
      "metadata": {
        "id": "01O5ZmLMZBf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RF User Behaviour Prediction and Projection"
      ],
      "metadata": {
        "id": "-a5MmPByZIYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Project RF\n",
        "import joblib\n",
        "rf_features = combined_train[[\"avg_post_per_day\", \"burstiness\", \"avg_tweet_length\"]]\n",
        "\n",
        "# 3. Load saved RF model\n",
        "rf_model_path = base_path / \"/content/drive/My Drive/ColabOutputs/\"user-model/user_model_v1\"\n",
        "rf_model = joblib.load(rf_model_path)\n",
        "\n",
        "# 4. Get predicted probabilities\n",
        "\n",
        "rf_probs = rf_model.predict_proba(rf_features)[:, 1].reshape(-1, 1)\n",
        "\n",
        "# Project to 128D\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "rf_projector = Sequential([\n",
        "    Input(shape=(1,)),\n",
        "    Dense(128, activation='relu')\n",
        "])\n",
        "rf_proj = rf_projector.predict(rf_probs)"
      ],
      "metadata": {
        "id": "Gd98iPV6ZDe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Set Preprocessing"
      ],
      "metadata": {
        "id": "H_cBRRUEZqsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User Metrics"
      ],
      "metadata": {
        "id": "tVhqsGZJZvVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(base_path / \"new_dev_cleaned_user.csv\")\n",
        "df['row_id'] = np.arange(len(df))\n",
        "df['Tweets'] = df['Tweets'].fillna('')\n",
        "df['datex'] = pd.to_datetime(df['datex'])\n",
        "df = df.dropna(subset=['io_flag'])"
      ],
      "metadata": {
        "id": "PHYy1qoZZuC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentenceBERT embedding\n",
        "user_grouped = df.groupby('User')['Tweets'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "user_grouped['embedding'] = user_grouped['Tweets'].apply(lambda x: model.encode(x))\n",
        "embedding_matrix = np.vstack(user_grouped['embedding'].values)\n",
        "\n",
        "# Post count per active day\n",
        "def active_days(dates):\n",
        "    return len(set(dates.dt.date))\n",
        "\n",
        "user_dates = df.groupby('User')['datex'].apply(list).reset_index()\n",
        "user_dates['active_days'] = user_dates['datex'].apply(lambda dates: active_days(pd.Series(dates)))\n",
        "user_dates['post_count'] = df.groupby('User').size().values\n",
        "user_dates['avg_post_per_day'] = user_dates['post_count'] / user_dates['active_days']\n",
        "user_grouped = user_grouped.merge(user_dates[['User', 'avg_post_per_day']], on='User')\n",
        "\n",
        "# Burstiness and average tweet length\n",
        "def compute_burstiness(dates):\n",
        "    if len(dates) < 2:\n",
        "        return 0\n",
        "    dates_sorted = sorted(dates)\n",
        "    gaps = [(dates_sorted[i+1] - dates_sorted[i]).total_seconds() / 3600 for i in range(len(dates_sorted)-1)]\n",
        "    return np.std(gaps)\n",
        "\n",
        "burstiness_df = df.groupby('User')['datex'].apply(compute_burstiness).reset_index(name='burstiness')\n",
        "avg_len_df = df.groupby('User')['Tweets'].apply(lambda x: np.mean(x.str.len())).reset_index(name='avg_tweet_length')\n",
        "behavior_df = burstiness_df.merge(avg_len_df, on='User')\n",
        "user_grouped = user_grouped.merge(behavior_df, on='User')\n",
        "\n"
      ],
      "metadata": {
        "id": "tmmrhV-zZx8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#('/content/drive/MyDrive/ColabOutputs/train_cleaned_tweet.csv', index=False)\n",
        "# Save\n",
        "np.save(base_path / 'new_user_embeddings_dev.npy', embedding_matrix)\n",
        "user_grouped.drop(columns='embedding').to_csv(base_path / 'new_user_features_dev.csv', index=False)\n",
        "behavior_df.to_csv(base_path / 'new_user_behavior_features_dev.csv', index=False)"
      ],
      "metadata": {
        "id": "8H9RfCYCZ3Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(base_path / \"new_dev_cleaned_user.csv\")\n",
        "df['Tweets'] = df['Tweets'].fillna('')\n",
        "df['datex'] = pd.to_datetime(df['datex'])\n",
        "\n",
        "user_grouped = df.groupby('User')['Tweets'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# avg_post_per_day\n",
        "def active_days(dates):\n",
        "    return len(set(dates.dt.date))\n",
        "user_dates = df.groupby('User')['datex'].apply(list).reset_index()\n",
        "user_dates['active_days'] = user_dates['datex'].apply(lambda d: active_days(pd.Series(d)))\n",
        "user_dates['post_count'] = df.groupby('User').size().values\n",
        "user_dates['avg_post_per_day'] = user_dates['post_count'] / user_dates['active_days']\n",
        "user_grouped = user_grouped.merge(user_dates[['User', 'avg_post_per_day']], on='User')\n",
        "\n",
        "# Burstiness & avg_len\n",
        "def compute_burstiness(dates):\n",
        "    if len(dates) < 2:\n",
        "        return 0\n",
        "    dates_sorted = sorted(dates)\n",
        "    gaps = [(dates_sorted[i+1] - dates_sorted[i]).total_seconds() / 3600 for i in range(len(dates_sorted)-1)]\n",
        "    return np.std(gaps)\n",
        "burstiness_df = df.groupby('User')['datex'].apply(compute_burstiness).reset_index(name='burstiness')\n",
        "avg_len_df = df.groupby('User')['Tweets'].apply(lambda x: np.mean(x.str.len())).reset_index(name='avg_tweet_length')\n",
        "behavior_df = burstiness_df.merge(avg_len_df, on='User')\n",
        "user_grouped = user_grouped.merge(behavior_df, on='User')\n",
        "\n",
        "labels = df[['User', 'io_flag']].dropna().drop_duplicates()\n",
        "labels = labels.groupby('User')['io_flag'].max().reset_index()\n",
        "user_grouped = user_grouped.merge(labels, on='User')\n"
      ],
      "metadata": {
        "id": "_p58Cwp9Z6Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#('/content/drive/MyDrive/ColabOutputs/train_cleaned_tweet.csv', index=False)\n",
        "user_grouped.to_csv(base_path / \"new_user_features_labeled_dev.csv\", index=False)"
      ],
      "metadata": {
        "id": "8PlJ9j9qZ-XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = pd.read_csv(base_path / \"new_user_features_labeled_dev.csv\")\n",
        "user_dev = dev[['User','avg_post_per_day',\t'burstiness',\t'avg_tweet_length']]\n",
        "combined_dev = pd.merge(dev_df, user_dev, on='User', how='left')"
      ],
      "metadata": {
        "id": "dbQ-ToXEaOB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model location using base path\n",
        "model_name = \"bertweet_metrics_f1_threshold70\"\n",
        "model_path = base_path / model_name\n",
        "\n",
        "# Load tokenizer and model from local path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Use AutoModel if extracting embeddings; use AutoModelForSequenceClassification for classification\n",
        "model = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "cbb4Y6XpahcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_dev = combined_dev['Tweets'].tolist()\n",
        "bertweet_embeddings_dev = get_bertweet_embeddings(texts_dev)  # shape (n, 768)"
      ],
      "metadata": {
        "id": "tq4ewoiiaRR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(base_path / 'bertweet_embeddings_dev.npy', bertweet_embeddings_dev)"
      ],
      "metadata": {
        "id": "yWWg25WNaVoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_embeddings_dev = np.load(base_path / 'bertweet_embeddings_dev.npy')"
      ],
      "metadata": {
        "id": "DLJ5vFajcjwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Project RF\n",
        "rf_features_dev = combined_dev[[\"avg_post_per_day\", \"burstiness\", \"avg_tweet_length\"]]\n",
        "\n",
        "# 3. Load saved RF model\n",
        "rf_model_path = base_path / \"user-model/user_model_v1\"\n",
        "rf_model = joblib.load(rf_model_path)\n",
        "\n",
        "# 4. Get predicted probabilities\n",
        "\n",
        "rf_probs_dev = rf_model.predict_proba(rf_features_dev)[:, 1].reshape(-1, 1)\n",
        "\n",
        "# Project to 128D\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "rf_projector = Sequential([\n",
        "    Input(shape=(1,)),\n",
        "    Dense(128, activation='relu')\n",
        "])\n",
        "rf_proj_dev = rf_projector.predict(rf_probs_dev)"
      ],
      "metadata": {
        "id": "YyYVz97YckPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacked Model Training"
      ],
      "metadata": {
        "id": "aWNOUS7UZU_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train = combined_train['io_flag']\n",
        "\n",
        "# Concat and Split\n",
        "X_combined = np.concatenate([bertweet_embeddings, rf_proj], axis=1)  # shape: (n, 896)\n",
        "y_combined = labels_train  # binary label (0/1)\n",
        "\n",
        "X_val = np.concatenate([bertweet_embeddings_dev, rf_proj_dev], axis=1)  # shape: (n, 896)\n",
        "y_val = labels_dev  # binary label (0/1)"
      ],
      "metadata": {
        "id": "s-ltvfbPZO3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Final MLP\n",
        "meta_model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(896,)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "meta_model.compile(\n",
        "    optimizer=Adam(1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='new_mlp_concat_{epoch:02d}.keras',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "meta_model.fit(\n",
        "    X_combined, y_combined,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "id": "7iYejsj9bgUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "#Best epoch = 10 (high precision, moderate recall), epoch 8 (balanced precision and recall)\n",
        "\n",
        "\n",
        "# Define source and destination paths\n",
        "source = 'new_mlp_concat_03.keras'\n",
        "destination = base_path / 'new_mlp_stacked_03.keras'\n",
        "\n",
        "# Copy the file\n",
        "shutil.copy(source, destination)"
      ],
      "metadata": {
        "id": "2MYFoxuNfiy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Analysis\n",
        "Below is further analysis for finding the reason why BERTweet models experienced drop of recall in test set."
      ],
      "metadata": {
        "id": "RQRlQw8uMw2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Unmasked Test Dataset"
      ],
      "metadata": {
        "id": "aBmseFsZPUCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv(base_path / \"new_test_cleaned.csv\")\n",
        "test = test_df.dropna(subset=['io_flag'])"
      ],
      "metadata": {
        "id": "otz6DnyrNdxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Embedding for Validation Set and Test Set\n",
        "Although this notebook doesn't include the generation of embeddings for test dataset, this can be done by following the same flow for train/validation dataset but using \"new_test_cleaned.csv\" as the datasource.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PdjMtuPkPYcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "X_val = np.load(base_path / \"bertweet_embeddings_dev.npy\")\n",
        "X_test = np.load(base_path / \"bertweet_embeddings_test.npy\")\n",
        "model = load_model(base_path / \"new_mlp_baseline_09.keras\")"
      ],
      "metadata": {
        "id": "kaTHRGn5NEqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Produce Prediction Using Model 2"
      ],
      "metadata": {
        "id": "idEBxNmrP03N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proba_val = model.predict(X_val).flatten()\n",
        "proba_test = model.predict(X_test).flatten()"
      ],
      "metadata": {
        "id": "aua54nfqNNmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = dev_df['io_flag']  # binary true labels\n",
        "y_test = test['io_flag']\n",
        "\n",
        "date_val = pd.to_datetime(dev_df['datex'])  # or your date column\n",
        "date_test = pd.to_datetime(test['datex'])\n",
        "\n",
        "\n",
        "df_val = pd.DataFrame({\n",
        "    \"date\": date_val,\n",
        "    \"proba\": proba_val,\n",
        "    \"true\": y_val,\n",
        "    \"set\": \"val\"\n",
        "})\n",
        "\n",
        "df_test = pd.DataFrame({\n",
        "    \"date\": date_test,\n",
        "    \"proba\": proba_test,\n",
        "    \"true\": y_test,\n",
        "    \"set\": \"test\"\n",
        "})\n",
        "\n",
        "df_all = pd.concat([df_val, df_test], ignore_index=True)\n",
        "df_all[\"pred\"] = (df_all[\"proba\"] >= 0.5).astype(int)\n",
        "df_all[\"correct\"] = df_all[\"pred\"] == df_all[\"true\"]\n",
        "df_all = df_all.sort_values(\"date\")\n"
      ],
      "metadata": {
        "id": "M_6o1AE8NqBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot The Prediction Probability Over Validation and Test"
      ],
      "metadata": {
        "id": "9A3YRonyQJL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare IO\n",
        "df_io = df_all[df_all[\"true\"] == 1].copy()\n",
        "df_io[\"day\"] = df_io[\"date\"].dt.date\n",
        "daily_stats_io = df_io.groupby(\"day\")[\"proba\"].agg([\"mean\", \"median\"])\n",
        "\n",
        "# Prepare Non-IO\n",
        "df_nonio = df_all[df_all[\"true\"] == 0].copy()\n",
        "df_nonio[\"day\"] = df_nonio[\"date\"].dt.date\n",
        "daily_stats_nonio = df_nonio.groupby(\"day\")[\"proba\"].agg([\"mean\", \"median\"])\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
        "\n",
        "# IO Plot\n",
        "axes[0].plot(daily_stats_io.index, daily_stats_io[\"mean\"], marker='o', color='purple', label=\"Mean\")\n",
        "axes[0].plot(daily_stats_io.index, daily_stats_io[\"median\"], marker='s', color='orange', label=\"Median\")\n",
        "axes[0].axvline(pd.to_datetime(\"2016-11-06\"), color=\"black\", linestyle=\"--\", label=\"Test Start\")\n",
        "axes[0].set_title(\"IO (true=1) Prediction Confidence\")\n",
        "axes[0].set_xlabel(\"Date\")\n",
        "axes[0].set_ylabel(\"Predicted Probability\")\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].legend()\n",
        "\n",
        "# Non-IO Plot\n",
        "axes[1].plot(daily_stats_nonio.index, daily_stats_nonio[\"mean\"], marker='o', color='purple', label=\"Mean\")\n",
        "axes[1].plot(daily_stats_nonio.index, daily_stats_nonio[\"median\"], marker='s', color='orange', label=\"Median\")\n",
        "axes[1].axvline(pd.to_datetime(\"2016-11-06\"), color=\"black\", linestyle=\"--\", label=\"Test Start\")\n",
        "axes[1].set_title(\"Non-IO (true=0) Prediction Confidence\")\n",
        "axes[1].set_xlabel(\"Date\")\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p4kX0GBlMv3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Count IO\n",
        "We checked this to know if the decrease of probability power is due to instability due to small number of observations or not"
      ],
      "metadata": {
        "id": "767s18CRQSkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dip due to small number?\n",
        "# Count IO (true=1) and Non-IO (true=0) samples per date\n",
        "counts_io = df_all[df_all[\"true\"] == 1].groupby(\"date\").size().rename(\"IO_count\")\n",
        "counts_nonio = df_all[df_all[\"true\"] == 0].groupby(\"date\").size().rename(\"NonIO_count\")\n",
        "\n",
        "# Combine and fill missing dates\n",
        "counts_df = pd.concat([counts_io, counts_nonio], axis=1).fillna(0).astype(int)\n",
        "\n",
        "# Print the result\n",
        "print(counts_df)\n"
      ],
      "metadata": {
        "id": "URCjlgLCN14j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Resize the plot\n",
        "fig, ax = plt.subplots(figsize=(7, 4))  # moderate width\n",
        "\n",
        "# Bar chart for IO tweet count\n",
        "ax.bar(counts_df.index, counts_df['IO_count'], color='red', label='IO Count (true=1)')\n",
        "\n",
        "# Add vertical line for test start date\n",
        "test_start_date = pd.to_datetime('2016-11-06')\n",
        "ax.axvline(test_start_date, color='black', linestyle='--', label='Test Start')\n",
        "\n",
        "# Format the x-axis\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add labels and legend\n",
        "ax.set_title('Daily IO (true=1) Tweet Count')\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Tweet Count')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2G_iYO3TOWzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Change of Vocabularies\n",
        "We perform this check to know the possibility of semantic shift on test period"
      ],
      "metadata": {
        "id": "k9wrpkFgObyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_unmasked = pd.read_csv(base_path / 'test_cleaned.csv')"
      ],
      "metadata": {
        "id": "QvOs6Ik7OgoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : copy train data\n",
        "train_copy = test_unmasked.copy()\n",
        "\n",
        "# step 2: Preprocess text for word frequency analysis\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_for_freq(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply to all training texts and flatten the token list\n",
        "all_tokens = train_copy['Tweets'].apply(preprocess_for_freq)\n",
        "flat_tokens = [token for sublist in all_tokens for token in sublist]\n",
        "\n"
      ],
      "metadata": {
        "id": "xOcXkR4UOmrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Get Top 50 Most Frequent Words\n",
        "word_freq = Counter(flat_tokens)\n",
        "top_words = [word for word, count in word_freq.most_common(50)]\n",
        "\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "m4gQIX0sOsg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## These are the identified topic specific words on train dataset\n",
        "topic_words = ['trump', 'clinton', 'hillary', 'donald', 'debate', 'vote', 'debatenight', 'trumps', 'gop', 'president', 'obama', 'election', 'bill', 'america', 'campaign', 'maga']\n"
      ],
      "metadata": {
        "id": "gQL9yXcEOuGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to sets for comparison\n",
        "top_words_set = set(top_words)\n",
        "topic_words_set = set(topic_words)\n",
        "\n",
        "# Find top words that are not in topic_words\n",
        "unrelated_words = top_words_set - topic_words_set\n",
        "\n",
        "# Print them\n",
        "print(sorted(unrelated_words))\n"
      ],
      "metadata": {
        "id": "0QYAjIC5OwnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot The Overlap Between Topic Specific Words in Test vs Train Dataset"
      ],
      "metadata": {
        "id": "Y8Tnzjm8Qu9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# Define custom stopwords to exclude\n",
        "custom_stopwords = {'dont', 'like', 'user', 'im', 'go', 'get', 'know', 'think', 'going', 'make', 'would',\n",
        "                    'cant', 'one', 'today', 'us', 'rt', 'see', 'want', 'time', 'u', '2', '4', 'day', 'country',\n",
        "                    'tomorrow', 'amp', 'httpurl', 'lets', 'got', 'new'}\n",
        "\n",
        "# Filter and sort top words\n",
        "filtered_word_freq = {word: freq for word, freq in word_freq.items() if word not in custom_stopwords}\n",
        "top_words_sorted = sorted(filtered_word_freq.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "words, freqs = zip(*top_words_sorted)\n",
        "colors = ['gray' if word in topic_words else 'red' for word in words]\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(words, freqs, color=colors)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Topical Words in Test Set')\n",
        "\n",
        "# Add legend\n",
        "legend_handles = [Patch(color='gray', label='Identified/masked in train set'),\n",
        "                  Patch(color='red', label='Not identified in train set')]\n",
        "plt.legend(handles=legend_handles)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "btLk0ivBO4P_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}